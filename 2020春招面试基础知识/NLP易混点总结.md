# 深度学习

### 1. 过拟合问题和欠拟合

**定义：**过拟合主要是由于模型太过复杂或者数据的问题，导致模型在训练的过程中表现的非常好，精度比较高，也就是对训练数据拟合的比较好。但是在测试集上进行评判时模型的表现很差。欠拟合就是对训练集的拟合也不是很好。

**解决方案：**

- L1和L2正则化
- dropout
- 提前停止
- 数据集扩增
- 简化网络结构
- 使用boosting或者bagging方法

### 2. 梯度消失和梯度爆炸

**概念和表现**：在反向求导的过程中，前面每层的梯度都是来自后面每层梯度的乘积，当层数过多时，有可能产生梯度不稳定，也就是梯度消失或者梯度爆炸，他门的本质都是因为梯度反向传播中的连乘效应。他的表现就是随着网络层数的加深，但是模型的效果却降低了。

**梯度消失产生的原因：** 隐藏层数量太大，使用了不合适的激活函数

**梯度爆炸产生的原因：**隐藏层数量太大，权重的初始化值过大，使用了不合适的激活函数

**如何解决**：

- 预训练加微调

- 加入正则化

- 梯度修剪

- 选择合适的激活函数，relu、leakrelu、elu等激活函数

- batchnorm

  Batchnorm本质上是解决反向传播过程中的梯度问题。batchnorm全名是batch normalization，简称BN，即批规范化，通过规范化操作把数据拉回到激活函数的梯度敏感区域，使得模型有一个更易于收敛。

- LSTM

  LSTM全称是长短期记忆网络（long-short term memory networks），是不那么容易发生梯度消失的，主要原因在于LSTM内部复杂的“门”(gates)，如下图，LSTM通过它内部的“门”可以接下来更新的时候“记住”前几次训练的”残留记忆“，因此，经常用于生成文本中。

- 选择合适的权重初始化手段

> 为什么ReLU可以避免梯度消失的问题
>
> ReLU的正半轴是线性的，他的导数是1且是一个固定值，所以容易避免发生梯度消失和梯度爆炸的问题。但是他并不能凶根本上解决梯度消失的问题，因为当输入是小于0的时候，就会把Relu的负半轴激活，在这一侧，ReLU的输出就是0，导数也是0， 他依旧无法避免梯度消失的问题。

### 3. GRU和LSTM的区别

### 4. CNN/RNN/LSTM/Transformer比较

> 1. [特征抽取器的比较（CNN/RNN/LSTM/Transformer）](https://zhuanlan.zhihu.com/p/54743941)

**RNN**

对于RNN，他主要能考虑到了序列信息，对于当前时刻输入的单词的词向量，他会利用tanh激活函数结合之前的文本序列信息来得到当前时刻的状态或者输出。在encoder-decoder模型中，他会把之前所有的序列信息加入到隐藏层来得到最后的decoder的输入。但是针对较短的文本序列问题，RNN可以有效的抓住之前的开始的文本序列信息。但是如果序列很长，这种线性序列结构在反向传播的时候容易导致严重的梯度消失或梯度爆炸问题

**LSTM**

LSTM的提出主要是为了解决长依赖问题和针对RNN的梯度消失的问题。LSTM的实现主要就是门机制，主要包括遗忘门，输入门，输出门三个门。在门控机制中的遗忘门中，通过sigmoid函数来决定需要遗忘上一个状态中哪些信息，在输入门中通过sigmoid函数结合当前输入和上一时刻的状态输出来决定更新cell中的信息。从宏观上来看，LSTM中有一个cell单元贯穿始终，使得中间状态信息直接向后传播。因此这使得LSTM可以有效的解决长依赖问题。

**但是一个主要的问题是RNN和LSTM等序列模型在计算当前时刻的信息时需要加入之前一个时刻的的隐藏层状态信息。由于整个模型是一个序列依赖关系，这也就限制了整个模型必须是串行计算，无法并行计算。**

**CNN**

对于CNN，他的实现或者说特征提取主要靠卷积层来提取特征，池化层来选择特征，然后使用全连接层来分类。其中卷积层提取特征主要是依靠卷积核对输入做卷积就可以得到特征。那么池化层一般选择使用最大池化来提取最主要的特征，在后面的全连接层中根据提取到的特征进行分类或者其他一些操作。但是卷积层中的CNN因为卷积核的存在，他依旧类似于N-gram，但是Dilated CNN的出现，使得CNN不是连续捕获特征，而是类似于跳一跳方式扩大捕获特征的距离。对位置信息敏感的序列可以抛弃掉max_pooling层。相对于LSTM， **CNN的主要优势在于并行的能力**。首先对于某个卷积核来说，每个滑动窗口位置之间没有依赖关系，所以可以并行计算；另外，不同的卷积核之间也没什么相互影响，所以也可以并行计算。

**Transformer**

针对Transformer，首先他打破了CNN和RNN框架。原文中主要是介绍self-attention的作用，但是核multi-attention、FFNN等也有很大的作用。Transformer也是跟CNN一样限定了句子的最大长度，句子长度不足的采用0进行padding。 在self-attention中，他打破了序列信息，而是当前单词可以和句子中任意一个单词编码，集成到embedding里面去，此外transformer利用sin/cos位置函数来进行位置编码。（Bert等模型则给每个单词一个Position embedding，随机初始化的）。针对长距离依赖特征的问题，Self -attention天然就能解决这个问题，因为在集成信息的时候，当前单词和句子中任意单词都发生了联系。所以Transformer也是支持并行计算的。

### **5. CNN和LSTM如何选择？**

CNN 和 LSTM是深度学习中使用最多，最为经典的两个模型，在NLP中，他们都可以用于提取文本的上下文特征，实际任务中该如何选择呢？ 实际中我们主要考虑三个因素：

- 上下文的依赖长度：
  CNN 提取的是类似于n-gram的局部的上下文特征，LSTM由于三个门限单元的加入使得它能够处理提取更长的上下文特征
- 位置特征：
  CNN由于max_pooling, 移除了文本序列中的相对位置信息，LSTM的序列结构，使得LSTM天然保留了位置信息
- 效率：
  CNN 没有序列上的前后依赖问题，可以高效的并行运算，而lstm由于存在序列上的传递依赖问题，效率上相比CNN要低

综合上面三个因素的比较，CNN比较适合上下文依赖较短且对相对位置信息不敏感的场景，如情感分析，情感分析里的一些难点如双重否定，在上下文中的距离都离得不远，LSTM适合需要长距离特征，且依赖相对位置信息的场景。

### 6. 常见激活函数的优缺点

- sigmoid函数 

  **优点：**

  1. Sigmoid函数的输出在(0,1)之间，输出范围有限，优化稳定，可以用作输出层。
  2. 连续函数，便于求导。

  **缺点：**

  1. sigmoid函数在变量取绝对值非常大的正值或负值时会出现**饱和**现象，意味着函数会变得很平，并且对输入的微小改变会变得不敏感。在**反向传播**时，当梯度接近于0，权重基本不会更新，很容易就会出现**梯度消失**的情况，从而无法完成深层网络的训练。
  2. **sigmoid函数的输出不是0均值的**，会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响。
  3. **计算复杂度高**，因为sigmoid函数是指数形式。

- tanh函数

  Tanh函数是 0 均值的，因此实际应用中 Tanh 会比 sigmoid 更好。但是仍然存在**梯度饱和**与**exp计算**的问题。

- ReLU函数

  **优点：**

  1. 使用ReLU的SGD算法的收敛速度比 sigmoid 和 tanh 快。
  2. 在x>0区域上，不会出现梯度饱和、梯度消失的问题。
  3. 计算复杂度低，不需要进行指数运算，只要一个阈值就可以得到激活值。

  **缺点：**

  1. ReLU的输出**不是0均值**的。
  2. **Dead ReLU Problem(神经元坏死现象)**：ReLU在负数区域被kill的现象叫做dead relu。ReLU在训练的时很“脆弱”。在x<0时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应，导致相应参数永远不会被更新。

  **产生**这种现象的两个**原因**：参数初始化问题；learning rate太高导致在训练过程中参数更新太大。

  **解决方法**：采用Xavier初始化方法，以及避免将learning rate设置太大或使用adagrad等自动调节learning rate的算法。



### 7. Attention机制的计算过程

#### 7.1 传统Attention及其计算过程

**attention机制的数学表达式**

> 直接照抄的参考博客2中的内容

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200707004748634.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fhd3NlZHJmMTIzbGFsYQ==,size_16,color_FFFFFF,t_70)

> 1. 白话attention的计算过程
>
>    基于encoder模型我们可以得到encoder中的hidden state，假如当前的decoder的hidden state是$s_{t-1}$，我们可以计算每一个输入位置j与当前输出位置的关联性，比如执行一次矩阵相乘，假设执行的结果是$e_t$。接下来对其进行 softmax 操作就得到了attention的分布，针对得到的attention和原有的隐藏层状态执行加权求和的操作，加权求和的结果再和decoder端的前一步的输出共同计算下一个状态的输出。

- 基于encoder模型我们可以得到encoder中的hidden state$(h_1, h_2, ..., h_T)$
- 假如当前的decoder的hidden state是$s_{t-1}$，我们可以计算每一个输入位置 j 与当前输出位置的关联性（比如执行一次矩阵相乘，假设执行的结果是$e_t$）。既$$e_{tj}=a(s_{t-1}, h_j)$$写成向量的形式就是$$\vec{e_t}=(a(s_{t-1},h_1),...,a(s_{t-1},h_T))$$其中的 a 是一种相关性的运算符，例如$\vec{e_t}=\vec{s_{t-1}}^T\vec{h}$。
- 对$\vec{e_t}$进行softmax操作，将其normalize得到attention的分布，$\vec{\alpha_t}=softmax(\vec{e_t})$。利用 $\vec{\alpha_t}$ 我们可以进行加权求和得到相应的context vector $$\vec{c_t} = \sum_{j=1}^T\alpha_{tj}h_j$$
- 由此，我们可以计算decoder的下一个hidden state $$s_t = f(s_{t-1},y_{t-1},c_t)$$以及该位置的输出$p(y_t|y_1,...,y_{t-1}, \vec{x}) = g(y_{i-1}, s_i, c_i)$ （Tips: 这里不一定非要作用于decoder，也可以是其他的下游任务）

![img](https://uploadfiles.nowcoder.com/images/20190318/311436_1552881536175_D404B240266A897C983A190746BE361D)

![img](https://uploadfiles.nowcoder.com/images/20190318/311436_1552881521786_71742153361B51DC4550A6A1ACD20228)

![img](https://uploadfiles.nowcoder.com/images/20190318/311436_1552881506279_6FEA48CC51271979D1CABDFBBA63AC9C)

> 介绍下attention：
>
> attention主要是在encoder-decoder模型中出现，原本的encoder-decoder模型的输出会关注输入的全部信息，而attention是希望关注和当前输出相关的重点局部内容。他的本质其实就是一系列的权重。



#### 7.2 self-attention及其计算过程

**1. 什么是self-attention**

Self Attention与传统的Attention机制非常的不同：
        传统的Attention是基于source端和target端的隐变量（hidden state）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self-Attention不同，它分别在source端和target端进行，仅与source input或者target input自身相关的Self Attention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self Attention加入到target端得到的Attention中，捕捉 source 端和 target 端词与词之间的依赖关系。

**2. self-attention模块的结构**

![img](https://pic2.zhimg.com/80/v2-32eb6aa9e23b79784ed1ca22d3f9abf9_720w.jpg)



**3. 具体的计算内容**

​        对于self-attention来讲，Q(Query), K(Key), V(Value)三个矩阵均来自同一输入，首先我们要计算Q与K之间的点乘，然后为了防止其结果过大，会除以一个尺度标度$\sqrt{d_k}$ ，其中 $d_k$ 为一个query和key向量的维度。再利用softmax 操作将其结果归一化为概率分布，然后再乘以矩阵 V 就得到权重求和的表示。该操作可以表示为

 ![[公式]](https://www.zhihu.com/equation?tex=Attention%28Q%2CK%2CV%29+%3D+softmax%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%29V)



**4. self-attention和attention的不同**：

- 传统的Attention是基于source端和target端的隐变量（hidden state）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系，而忽略了源端或目标端句子中词与词之间的依赖关系
- 但Self Attention不同，它分别在source端和target端进行，捕捉source端或target端自身的词与词之间的依赖关系；然后可以再把source端的得到的信息加入到target端中，这也就捕捉source端和target端词与词之间的依赖关系。所以相对于传统的Attention，他可以获取序列本身词与词之间的依赖关系，也可以得到source端和target端之间的关系。

### 8. 决策树

**什么是决策树？**

决策树就是一种描述对实例进行分类的树形结构，他由结点和有向边组成。结点分为内部结点和叶节点，内部结点表示一个特征或者属性，叶节点表示一个类。决策树也可以看做是一个if-else规则的集合。决策树的根节点到叶节点的每一条路径构建一条规则，路径上内部结点的特征对应着规则的条件，而叶节点是对应着规则的结论。决策树算法有多种，但是无论哪一种都是**为了让模型的不确定降低的越快越好**，基于其评价指标的不同，主要分为**ID3算法，C4.5算法和CART算法**。其中**ID3算法的评价指标是信息增益，C4.5算法的评价指标是信息增益比，CART算法的评价指标是基尼系数**。

> **值得注意的是CART分类树采用的是基尼系数，而CART回归树采用的是选取可以使得误差最小的（j,s）对。(j-特征是值；s-选定的特征)**

#### 8.1 决策树常问面试题

- **谈谈自己对决策树的理解？**
  决策树算法，无论是哪种，其目的都是为了让模型的不确定性降低的越快越好，基于其评价指标的不同，主要是ID3算法，C4.5算法和CART算法，其中ID3算法的评价指标是信息增益，C4.5算法的评价指标是信息增益率，CART算法的评价指标是基尼系数。

  > **C4.5相对于ID3的优点：**
  >
  > ID3算法以信息增益为准则来选择决策树划分属性。值多的属性更有可能会带来更高的纯度提升，**所以信息增益的比较偏向选择取值多的属性**。所以为了解决这个问题就用了信息增益比。C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

- **谈谈对信息增益和信息增益率的理解？**

  - 要理解信息增益，**首先**要理解熵这个概念。从概率统计的角度看，熵是对随机变量不确定性的度量，也可以说是对随机变量的概率分布的一个衡量。**熵越大，随机变量的不确定性就越大。对同一个随机变量，当他的概率分布为均匀分布时，不确定性最大，熵也最大。对有相同概率分布的不同的随机变量，取值越多的随机变量熵越大**。**其次**，要理解条件熵的概念。正如熵是对随机变量不确定性的度量一样，条件熵是指，有相关的两个随机变量X和Y，在已知随机变量X的条件下，随机变量Y的不确定性。当熵和条件熵中概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别为**经验熵**与**经验条件熵**。
  - 所谓信息增益，也叫互信息，就是指集合D的经验熵H ( D ) H(D)*H*(*D*)与特征A给定条件下D的经验条件熵H ( D ∣ A ) H(D|A)*H*(*D*∣*A*)之差。ID3算法在每一次对决策树进行分叉选取最优特征时，会选取信息增益最高的特征来作为分裂特征。
  - 信息增益准则的问题（ID3算法存在的问题）？
    信息增益准则对那些特征的取值比较多的特征有所偏好，也就是说，采用信息增益作为判定方法，会倾向于去选择特征取值比较多的特征作为最优特征。那么，选择取值多的特征为甚就不好呢？参考[这篇博文](https://blog.csdn.net/u012351768/article/details/73469813)。
  - 采用信息增益率的算法C4.5为什么可以解决ID3算法中存在的问题呢？
    信息增益率的公式如下：
    g R ( D , A ) = g ( D , A ) H A ( D ) g_R(D,A) = \frac{g(D,A)}{H_A(D)}*g**R*​(*D*,*A*)=*H**A*​(*D*)*g*(*D*,*A*)​
    其中，H A ( D ) = − ∑ i = 1 n ∣ D i ∣ ∣ D ∣ l o g 2 ∣ D i ∣ ∣ D ∣ H_A(D) = -\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}*H**A*​(*D*)=−∑*i*=1*n*​∣*D*∣∣*D**i*​∣​*l**o**g*2​∣*D*∣∣*D**i*​∣​，n是特征A A*A*取值的个数。H A ( D ) H_A(D)*H**A*​(*D*)表示的就是特征A A*A*的纯度，如果A A*A*只含有少量的取值的话，那么A A*A*的纯度就比较高，H A ( D ) H_A(D)*H**A*​(*D*)就比较小；相反，如果A A*A*取值越多的话，那么A A*A*的纯度就越低，H A ( D ) H_A(D)*H**A*​(*D*)就比较大。这样就可以解决ID3算法中存在的问题了。

- **决策树出现过拟合的原因及其解决办法？**

  对训练数据预测效果很好，但是测试数据预测效果较差的现象称为过拟合。

  - 原因：
    - 在决策树构建的过程中，对决策树的生长没有进行合理的限制（剪枝）；
    - 样本中有一些噪声数据，没有对噪声数据进行有效的剔除；
    - 在构建决策树过程中使用了较多的输出变量，变量较多也容易产生过拟合。
  - 解决办法
    - 选择合理的参数进行剪枝，可以分为预剪枝和后剪枝，我们一般采用后剪枝的方法；
    - 利用 K-folds交叉验证，将训练集分为K份，然后进行K次交叉验证，每次使用K−1份作为训练样本数据集，另外一份作为测试集；
    - 减少特征，*计算每一个特征和响应变量的相关性，常见得为皮尔逊相关系数，将相关性较小的变量剔除*（待解释！！！）；*当然还有一些其他的方法来进行特征筛选，比如基于决策树的特征筛选，通过正则化的方式来进行特征选取等*（决策的正则化，例如，L1和L2正则，具体是对谁的正则呢？怎样正则的呢？）。**面试官顺便会问L1和L2，一定要搞明白**

- **简单解释一下预剪枝和后剪枝，以及剪枝过程中可以参考的参数有哪些？**

  - 预剪枝：在决策树生成初期就已经设置了决策树的参数，决策树构建过程中，满足参数条件就提前停止决策树的生成。
  - 后剪枝：后剪枝是一种全局的优化方法，它是在决策树完全建立之后再返回去对决策树进行剪枝。
  - 参数：树的高度、叶子节点的数目、最大叶子节点数、限制不纯度。

- **决策树的优缺点**

  - 优点：
    - **计算简单、速度快**；
    - **可解释性强**；
    - **比较适合处理有缺失属性的样本**。
  - 缺点：
    - **容易发生过拟合**（随机森林可以很大程度上减少过拟合）；
    - **忽略了数据之间的相关性**；
    - **对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征**（只要是使用了信息增益，都有这个缺点，如RF）。*对应的案例如下：有这么一个场景，在一个样本集中，其中有100个样本属于A，9900个样本属于B，用决策树算法实现对AB样本进行区分的时候，会发生欠拟合的现象。因为在这个样本集中，AB样本属于严重失衡状态，在建立决策树算法的过程中，模型会更多的偏倚到B样本的性质，对A样本的性质训练较差，不能很好的反映样本集的特征。*（待解释！！！）。

- **决策树是如何处理缺失值的？**

  > 1. [决策树（decision tree）（四）——缺失值处理](https://blog.csdn.net/u012328159/article/details/79413610)

- **决策树与逻辑回归的区别？**

  - 对于拥有**缺失值**的数据，决策树可以应对，而逻辑回归需要挖掘人员预先对缺失数据进行处理；

  - 逻辑回归对数据**整体结构**的分析优于决策树，而决策树对**局部结构**的分析优于逻辑回归；

    > 决策树由于采用分割的方法，所以能够深入数据内部，但同时失去了对全局的把握。一个分层一旦形成，它和别的层面或节点的关系就被切断了，以后的挖掘只能在局部中进行。同时由于切分，样本数量不断萎缩，所以无法支持对多变量的同时检验。而逻辑回归，始终着眼整个数据的拟合，所以对全局把握较好。但无法兼顾局部数据，或者说缺乏探查局部结构的内在机制。

  - 逻辑回归擅长分析**线性关系**，而决策树对线性关系的把握较差。线性关系在实践中有很多优点：简洁，易理解，可以在一定程度上防止对数据的过度拟合。

    > 我自己对线性的理解：
    >
    > 1. 逻辑回归应用的是样本数据线性可分的场景，输出结果是概率，即，输出结果和样本数据之间不存在直接的线性关系；
    > 2. 线性回归应用的是样本数据和输出结果之间存在线性关系的场景，即，自变量和因变量之间存在线性关系。

  - 逻辑回归对**极值**比较敏感，容易受极端值的影响，而决策树在这方面表现较好。

  - 应用上的区别：**决策树的结果和逻辑回归相比略显粗糙**。逻辑回归原则上可以提供数据中每个观察点的概率，而决策树只能把挖掘对象分为有限的概率组群。比如决策树确定17个节点，全部数据就只能有17个概率，在应用上受到一定限制。就操作来说，决策树比较容易上手，需要的数据预处理较少，而逻辑回归则要去一定的训练和技巧。

  - **执行速度**上：当数据量很大的时候，逻辑回归的执行速度非常慢，而决策树的运行速度明显快于逻辑回归。

- **扩展随机森林、GDBT等问题**

#### 8.2 CART回归树



> (j, s)--(切分变量<特征>，切分点<特征取值>)

**CART就是递归的把所有的数据集划分为N个域，然后取每个域中的平均值作为当前叶节点的输出。选择哪个(j, s) 对来作为当前结点中数据的的分裂依据是根据按照选定的(j, s)划分后，损失函数是不是最小。**



![在这里插入图片描述](https://img-blog.csdnimg.cn/20200520111110367.png)



### 9. 梯度下降优化算法

> https://zhuanlan.zhihu.com/p/55150256

**1. 梯度下降法(Gradient Descent)**

> 梯度下降法如何跳出局部最小值
>
> 1. 采用不同的初始化方式来初始化多个神经网络，然后进行训练。最后选择效果最好的神经网络的参数作为最佳的参数
> 2. 采用随机梯度下降法。SGD由于每次参数更新仅仅需要计算一个样本的梯度，训练速度很快，即使在样本量很大的情况下，可能只需要其中一部分样本就能迭代到最优解，由于每次迭代并不是都向着整体最优化方向，导致梯度下降的波动非常大，更容易从一个局部最优跳到另一个局部最优，准确度下降。
> 3. 使用模拟退火的方案。(这个是一个不熟悉的方案)





**2. 随机梯度下降(SGD)**



**3. 批梯度下降(BGD)**



**4. 小批量梯度下降(Mini-batch)**





### 10. 分类为什么使用交叉熵？

我们希望模型在训练数据上学到的**预测数据分布**与**真实数据分布**越相近越好，上面讲过了，用相对熵，但是为了简便计算使用交叉熵就可以。

> 为什么使用交叉熵而不是均方误差
>
> 从以上公式可以看出：**均方差**对参数的偏导的结果都**乘了sigmoid的导数** ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%27%28z%29x) ，而sigmoid导数在其变量值很大或很小时趋近于0，所以偏导数很有可能接近于0。
>
> 由参数更新公式：**参数=参数-学习率×损失函数对参数的偏导**
>
> 可知，偏导很小时，参数更新速度会变得很慢，而当偏导接近于0时，参数几乎就不更新了。反观**交叉熵**对参数的偏导就**没有sigmoid导数**，所以不存在这个问题。**这就是选择交叉熵而不选择均方差的原因。**







# 五、机器学习相关

## 1. bagging 和boosting的区别

**样本选择上：**

- Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
- Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

**样本权重上**

- Bagging：使用均匀取样，每个样例的权重相等
- Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

**预测函数：**

- Bagging：所有预测函数的权重相等。

- Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

**并行计算：**

- Bagging：各个预测函数可以并行生成
- Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

**偏差和方差**

- **Bagging是降低方差**
- **boosting是降低偏差**

> 典型的bagging算法：RF
>
> 典型的boosting算法：Adaboost， 提升树，GBDT，XGBoost

- Bagging 是 Bootstrap Aggregating的简称，意思就是再取样 (Bootstrap) 然后在每个样本上训练出来的模型取平均，所以是降低模型的**方差.** Bagging 比如 Random Forest 这种先天并行的算法都有这个效果。
- Boosting 则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行加权，所以随着迭代不不断进行行，误差会越来越小，所以模型的**偏差**会不不断降低。这种算法无法并行。

## 2. 决策树

**什么是决策树？**

决策树就是一种描述对实例进行分类的树形结构，他由结点和有向边组成。结点分为内部结点和叶节点，内部结点表示一个特征或者属性，叶节点表示一个类。决策树也可以看做是一个if-else规则的集合。决策树的根节点到叶节点的每一条路径构建一条规则，路径上内部结点的特征对应着规则的条件，而叶节点是对应着规则的结论。决策树算法有多种，但是无论哪一种都是**为了让模型的不确定降低的越快越好**，基于其评价指标的不同，主要分为**ID3算法，C4.5算法和CART算法**。其中**ID3算法的评价指标是信息增益，C4.5算法的评价指标是信息增益比，CART算法的评价指标是基尼系数**。

> **值得注意的是CART分类树采用的是基尼系数，而CART回归树采用的是选取可以使得误差最小的（j,s）对。(j-特征是值；s-选定的特征)**

### 2.1 决策树常问面试题

- **谈谈自己对决策树的理解？**
  决策树算法，无论是哪种，其目的都是为了让模型的不确定性降低的越快越好，基于其评价指标的不同，主要是ID3算法，C4.5算法和CART算法，其中ID3算法的评价指标是信息增益，C4.5算法的评价指标是信息增益率，CART算法的评价指标是基尼系数。

  > C4.5相对于ID3的优点：
  >
  > ID3算法以信息增益为准则来选择决策树划分属性。值多的属性更有可能会带来更高的纯度提升，**所以信息增益的比较偏向选择取值多的属性**。所以为了解决这个问题就用了信息增益比。C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

- **谈谈对信息增益和信息增益率的理解？**

  - 要理解信息增益，**首先**要理解熵这个概念。从概率统计的角度看，熵是对随机变量不确定性的度量，也可以说是对随机变量的概率分布的一个衡量。**熵越大，随机变量的不确定性就越大。对同一个随机变量，当他的概率分布为均匀分布时，不确定性最大，熵也最大。对有相同概率分布的不同的随机变量，取值越多的随机变量熵越大**。**其次**，要理解条件熵的概念。正如熵是对随机变量不确定性的度量一样，条件熵是指，有相关的两个随机变量X和Y，在已知随机变量X的条件下，随机变量Y的不确定性。当熵和条件熵中概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别为**经验熵**与**经验条件熵**。
  - 所谓信息增益，也叫互信息，就是指集合D的经验熵H ( D ) H(D)*H*(*D*)与特征A给定条件下D的经验条件熵H ( D ∣ A ) H(D|A)*H*(*D*∣*A*)之差。ID3算法在每一次对决策树进行分叉选取最优特征时，会选取信息增益最高的特征来作为分裂特征。
  - 信息增益准则的问题（ID3算法存在的问题）？
    信息增益准则对那些特征的取值比较多的特征有所偏好，也就是说，采用信息增益作为判定方法，会倾向于去选择特征取值比较多的特征作为最优特征。那么，选择取值多的特征为甚就不好呢？参考[这篇博文](https://blog.csdn.net/u012351768/article/details/73469813)。
  - 采用信息增益率的算法C4.5为什么可以解决ID3算法中存在的问题呢？
    信息增益率的公式如下：
    g R ( D , A ) = g ( D , A ) H A ( D ) g_R(D,A) = \frac{g(D,A)}{H_A(D)}*g**R*​(*D*,*A*)=*H**A*​(*D*)*g*(*D*,*A*)​
    其中，H A ( D ) = − ∑ i = 1 n ∣ D i ∣ ∣ D ∣ l o g 2 ∣ D i ∣ ∣ D ∣ H_A(D) = -\sum_{i=1}^n\frac{|D_i|}{|D|}log_2\frac{|D_i|}{|D|}*H**A*​(*D*)=−∑*i*=1*n*​∣*D*∣∣*D**i*​∣​*l**o**g*2​∣*D*∣∣*D**i*​∣​，n是特征A A*A*取值的个数。H A ( D ) H_A(D)*H**A*​(*D*)表示的就是特征A A*A*的纯度，如果A A*A*只含有少量的取值的话，那么A A*A*的纯度就比较高，H A ( D ) H_A(D)*H**A*​(*D*)就比较小；相反，如果A A*A*取值越多的话，那么A A*A*的纯度就越低，H A ( D ) H_A(D)*H**A*​(*D*)就比较大。这样就可以解决ID3算法中存在的问题了。

- **决策树出现过拟合的原因及其解决办法？**

  对训练数据预测效果很好，但是测试数据预测效果较差的现象称为过拟合。

  - 原因：
    - 在决策树构建的过程中，对决策树的生长没有进行合理的限制（剪枝）；
    - 样本中有一些噪声数据，没有对噪声数据进行有效的剔除；
    - 在构建决策树过程中使用了较多的输出变量，变量较多也容易产生过拟合。
  - 解决办法
    - 选择合理的参数进行剪枝，可以分为预剪枝和后剪枝，我们一般采用后剪枝的方法；
    - 利用 K-folds交叉验证，将训练集分为K份，然后进行K次交叉验证，每次使用K−1份作为训练样本数据集，另外一份作为测试集；
    - 减少特征，*计算每一个特征和响应变量的相关性，常见得为皮尔逊相关系数，将相关性较小的变量剔除*（待解释！！！）；*当然还有一些其他的方法来进行特征筛选，比如基于决策树的特征筛选，通过正则化的方式来进行特征选取等*（决策的正则化，例如，L1和L2正则，具体是对谁的正则呢？怎样正则的呢？）。**面试官顺便会问L1和L2，一定要搞明白**

- **简单解释一下预剪枝和后剪枝，以及剪枝过程中可以参考的参数有哪些？**

  - 预剪枝：在决策树生成初期就已经设置了决策树的参数，决策树构建过程中，满足参数条件就提前停止决策树的生成。
  - 后剪枝：后剪枝是一种全局的优化方法，它是在决策树完全建立之后再返回去对决策树进行剪枝。
  - 参数：树的高度、叶子节点的数目、最大叶子节点数、限制不纯度。

- **决策树的优缺点**

  - 优点：
    - **计算简单、速度快**；
    - **可解释性强**；
    - **比较适合处理有缺失属性的样本**。
  - 缺点：
    - **容易发生过拟合**（随机森林可以很大程度上减少过拟合）；
    - **忽略了数据之间的相关性**；
    - **对于那些各类别样本数量不一致的数据，在决策树当中,信息增益的结果偏向于那些具有更多数值的特征**（只要是使用了信息增益，都有这个缺点，如RF）。*对应的案例如下：有这么一个场景，在一个样本集中，其中有100个样本属于A，9900个样本属于B，用决策树算法实现对AB样本进行区分的时候，会发生欠拟合的现象。因为在这个样本集中，AB样本属于严重失衡状态，在建立决策树算法的过程中，模型会更多的偏倚到B样本的性质，对A样本的性质训练较差，不能很好的反映样本集的特征。*（待解释！！！）。

- **决策树是如何处理缺失值的？**

  > 1. [决策树（decision tree）（四）——缺失值处理](https://blog.csdn.net/u012328159/article/details/79413610)

- **决策树与逻辑回归的区别？**

  - 对于拥有**缺失值**的数据，决策树可以应对，而逻辑回归需要挖掘人员预先对缺失数据进行处理；

  - 逻辑回归对数据**整体结构**的分析优于决策树，而决策树对**局部结构**的分析优于逻辑回归；

    > 决策树由于采用分割的方法，所以能够深入数据内部，但同时失去了对全局的把握。一个分层一旦形成，它和别的层面或节点的关系就被切断了，以后的挖掘只能在局部中进行。同时由于切分，样本数量不断萎缩，所以无法支持对多变量的同时检验。而逻辑回归，始终着眼整个数据的拟合，所以对全局把握较好。但无法兼顾局部数据，或者说缺乏探查局部结构的内在机制。

  - 逻辑回归擅长分析**线性关系**，而决策树对线性关系的把握较差。线性关系在实践中有很多优点：简洁，易理解，可以在一定程度上防止对数据的过度拟合。

    > 我自己对线性的理解：
    >
    > 1. 逻辑回归应用的是样本数据线性可分的场景，输出结果是概率，即，输出结果和样本数据之间不存在直接的线性关系；
    > 2. 线性回归应用的是样本数据和输出结果之间存在线性关系的场景，即，自变量和因变量之间存在线性关系。

  - 逻辑回归对**极值**比较敏感，容易受极端值的影响，而决策树在这方面表现较好。

  - 应用上的区别：**决策树的结果和逻辑回归相比略显粗糙**。逻辑回归原则上可以提供数据中每个观察点的概率，而决策树只能把挖掘对象分为有限的概率组群。比如决策树确定17个节点，全部数据就只能有17个概率，在应用上受到一定限制。就操作来说，决策树比较容易上手，需要的数据预处理较少，而逻辑回归则要去一定的训练和技巧。

  - **执行速度**上：当数据量很大的时候，逻辑回归的执行速度非常慢，而决策树的运行速度明显快于逻辑回归。

- **扩展随机森林、GDBT等问题**



### 2.1 CART回归树



> (j, s)--(切分变量<特征>，切分点<特征取值>)

**CART就是递归的把所有的数据集划分为N个域，然后取每个域中的平均值作为当前叶节点的输出。选择哪个(j, s) 对来作为当前结点中数据的的分裂依据是根据按照选定的(j, s)划分后，损失函数是不是最小。**



![在这里插入图片描述](https://img-blog.csdnimg.cn/20200520111110367.png)





## 3. Bagging算法：随机森林

> 随机森林本质上就是利用数据集训练多颗决策树。在预测的时候利用所有决策树中的众数来作为随机森林最后的输出。

**随机森林算法**

- 采取有放回的方法利用原来的数据集构建N个数据集，并利用这个N个数据集训练N颗决策树
- 当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。
- 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。
- 按照以上过程构建大量的决策树组成随机森林。



> 1. 随机森林的随机体现在什么地方？
>    - 在训练弱分类器时选取的训练集是随机的；
>    - 列抽样时选择的特征是随机的.

## 4. boosting算法：提升树

> 提升树是boosting算法的一种，本质上可以理解为是一个加法模型，他的基分类器是回归树模型。他的损失函数是均方误差损失函数，从公式的推到中可以看出。提升树的本质就是拟合残差。

**提升树算法**

$$f_m(x) = f_{m-1}(x) + T(x;\theta_{m})$$

$$\hat{\theta}_{m} = argmin\sum{^N_{i=1}}(y_i, f_{m-1}(x_i)+T(x_i;\theta_{m}))$$

当采用均方误差损失函数的时候，其loss函数变为：



$$L(y,f_{m-1}(x)+T(x;\theta_{m}))\\=[y-f_{m-1}(x)-T(x;\theta_{m})]^2\\=[\gamma-T(x;\theta_{m})]$$



其中$f_{m-1}(x)$是之前得到的树的预测值；$\gamma$ 是残差（残差就可以使用回归树来拟合）；

## 5. boosting算法：Adaboost

Adaboost是一种自适应boosting算法，他的自适应体现在每训练完一个分类器，他会根据训练结果更新当前的数据集的权重。增大被错误分类的数据的权重，减少被正确分类的数据，然后利用更新以后的数据再次训练一个弱分类器并加入到之前得到的强分类器里。如此循环训练直到错误率达到某一个比较的值或者最大的迭代次数。

**5.1 整个Adaboost 迭代算法就3步：**

- 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。
- 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。
- 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。

**5.2 Adaboost算法流程**

给定一个训练数据集T={(x1,y1), (x2,y2)…(xN,yN)}，yi属于标记集合{-1,+1}，Adaboost的目的就是从训练数据中学习一系列弱分类器或基本分类器，然后将这些弱分类器组合成一个强分类器。

**步骤1**. 首先，初始化训练数据的权值分布。每一个训练样本最开始时都被赋予相同的权值：1/N。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200516164006490.png)



**步骤2**. 进行多轮迭代，用m = 1,2, ..., M表示迭代的第多少轮
**a.** 使用具有权值分布Dm的训练数据集学习，得到基本分类器（选取让误差率最低的阈值来设计基本分类器）：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200516164040172.png)

**b.** 计算Gm(x)在训练数据集上的分类误差率

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200516164055965.png)

由上述式子可知，Gm(x)在训练数据集上的误差率em就是被Gm(x)误分类样本的权值之和。

**c.** 计算Gm(x)的系数，am表示Gm(x)在最终分类器中的重要程度（目的：得到基本分类器在最终分类器中所占的权重。注：这个公式写成$\alpha_m=1/2ln((1-e_m)/e_m)$更准确，因为底数是自然对数e，故用In，写成log容易让人误以为底数是2或别的底数，下同）：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200516164150444.png)

由上述式子可知，$e_m <= 1/2$时，$\alpha_m >= 0$，且$\alpha_m$随着$e_m$的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。

**d.** 更新训练数据集的权值分布（目的：得到样本的新的权值分布），用于下一轮迭代

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200516164406980.png)

使得被基本分类器Gm(x)误分类样本的权值增大，而被正确分类样本的权值减小。就这样，通过这样的方式，AdaBoost方法能“重点关注”或“聚焦于”那些较难分的样本上。

 其中，Zm是规范化因子，使得Dm+1成为一个概率分布：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200516165117686.png)

步骤3. 组合各个弱分类器

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020051616512730.png)

从而得到最终分类器，如下：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200516165135121.png)

## 6. boosting算法：GBDT

GBDT应该和提升树联系在一起。提升树的损失函数是均方误差损失函数，此时就相当于利用一颗回归树来拟合当前数据的残差，但是当损失函数不再是均方误差的时候，提升树就显得无能为力了。因此为了扩展到其他损失函数上，提出了GBDT算法。当GBDT算法采用均方损失的时候，GBDT和提升树就很接近了。GBDT是借鉴于梯度下降法，其基本原理是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中，他的基分类器是采用回归树。

<font color=red>**GBDT拓展到用弱分类器来拟合负梯度主要是利用了一阶泰勒展开式来看，从泰勒展开式的第二项看，如果要使得损失函数下降，只需要使得要训练的基分类器的输出在数值上等于损失函数在当前点的负值就可以。也就是利用基分类器去拟合负梯度。**</font>

**6.1 GBDT拟合的为什么是负梯度**

**优化目标函数**：$\sum{_{i=1}^N}L(y_{i},h_{m-1}(x_{i})+f_{m}(x_{i}))$
最小化上述目标函数，也就是每添加一个弱分类器就使得损失函数下降一部分。利用泰勒公式对上述问题进行近似来回答为什么GBDT拟合的是负梯度
$$L(y_{i},h_{m-1}(x_{i})+f_{m}(x_{i})) = \\L(y_{i},h_{m-1}(x_{i}))+ \frac{\partial{L(y_{i},f_{m-1}(x_{i}))}}{\partial(f_{m-1}(x))}*f_{m}(x_{i})$$
当
$$f_{m}(x_{i}) = -\frac{\partial{L(y_{i},f_{m-1}(x_{i}))}}{\partial(f_{m-1}(x))}$$则肯定有$$L(y_{i},h_{m-1}(x_{i})+f_{m}(x_{i})) <L(y_{i},h_{m-1}(x_{i}))$$
也就是利用新的弱分类器取拟合当前损失函数的负梯度就会使得整个损失函数不断减小。当损失函数是平方损失的时候，负梯度就是残差，也就是说拟合残差是GBDT中的一种特殊情况。



![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727160509955.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fhd3NlZHJmMTIzbGFsYQ==,size_16,color_FFFFFF,t_70)



![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072716051056.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fhd3NlZHJmMTIzbGFsYQ==,size_16,color_FFFFFF,t_70)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727160509616.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fhd3NlZHJmMTIzbGFsYQ==,size_16,color_FFFFFF,t_70)



**6.2 在GBDT中如何确定当前的点是否要分裂的依据**

我们可以把损失函数由每个样本的表达形式转化为每个叶节点表达的形式，由此可以计算出该叶节点分裂前后的损失函数减少值，如果该值的减少符合预期就对当前结点进行分裂，否则就不分裂。

**6.3 QA**

**Q1. GBDT为什么是拟合负梯度**
GBDT本身就是一个回归算法，回归算法的本质就是最小化损失函数，而最小化损失函数的本质又是梯度下降。这里采用平方差和作为损失函数，其求导正好是残差，所以就相当于是利用提升树来集合残差。

**Q2. 在GBDT中为什么使用泰勒公式推导梯度下降算法**
泰勒公式推导只是一种方法

**Q3. GBDT和提升树区别**
提升树模型每一次的提升都是靠上次的预测结果与训练数据的label值差值作为新的训练数据进行重新训练，由于原始的回归树指定了平方损失函数所以可以直接计算残差，**而梯度提升树针对一般损失函数**，所以**采用负梯度来近似求解残差**，将残差计算替换成了损失函数的梯度方向，将上一次的预测结果带入梯度中求出本轮的训练数据。**这两种模型就是在生成新的训练数据时采用了不同的方法**。

**Q4. GBDT是如何防止过拟合的**

- 为了防止过拟合，在GBDT中用到了Shrinkage–缩减，循序渐进：Shrinkage的思想强调的是循序渐进，就好比不能一口吃出一个胖子。每次迭代只走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易避免过拟合。即它不完全信任每一个棵残差树，它认为每棵树只学到了真理的一小部分，累加的时候只累加一小部分，通过多学几棵树弥补不足。在[参考博文一](https://blog.csdn.net/zpalyq110/article/details/79527653)中使用了这个技巧。也就是最后的强学习构造中给予了后面学到的每棵树一个权重。$f_{M}(x) = f_{0}(x) + lr*\sum{_{i=1}^M}\sum{_{j=1}^N}T(x,\theta)$
- 控制迭代的次数，也就是控制生成的树的数量
- 控制叶子节点中最少的样本个数
- 控制树的复杂性

**Q5. GBDT是如何实现正则化的**

- 第一种是和Adaboost类似的正则化项，即步长(learning rate)。也就是shrinkage来确定每一步学到的知识的多少
- 第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]
- 第三种是对于弱学习器即CART回归树进行正则化剪枝

**Q6. GBDT的优缺点**
　　**GBDT主要的优点有**：
　　1) 可以灵活处理各种类型的数据，包括连续值和离散值。
　　2) 在相对少的调参时间情况下，预测的准备率也可以比较高。这个是相对SVM来说的。
        3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。
　　**GBDT的主要缺点有：**
　　1)由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。

## 7. boosting算法：XGBoost

> 1. [Python机器学习笔记：XgBoost算法](https://www.cnblogs.com/wj-1314/p/9402324.html)
> 2. [XGBoost和GBDT的不同](https://www.zhihu.com/question/41354392)
> 3. [XGBoost的详细公式推导](https://zhuanlan.zhihu.com/p/29765582)
> 4. [目前看到的对XGBoost最好的解读](https://zhuanlan.zhihu.com/p/92837676)
> 5. [20道XGBoost面试题](https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&mid=2247485159&idx=1&sn=d429aac8370ca5127e1e786995d4e8ec&chksm=e9d01626dea79f30043ab80652c4a859760c1ebc0d602e58e13490bf525ad7608a9610495b3d&scene=21#wechat_redirect)

### **7.1 XGBoost和GBDT介绍**

XGboost和GBDT是boosting算法的一种，XGBoost其本质上还是一个GBDT的工程实现，但是力争把速度和效率发挥到极致。

GBDT算法本身也是一种加法模型，是对提升树一种优化。他使得boosting算法可以拓展到应对任何损失函数类别。理论中，针对GBDT的损失函数做了一个一阶泰勒近似，一阶泰勒近似的结果就是一个一阶导数，也就是梯度。因此本质上GBDT是对损失函数的负梯度的一个拟合，当损失函数采用均方误差损失的时候，GBDT拟合的负梯度就是残差。在这个过程中，GBDT使用的基分类器是CART回归树。

对于XGBoost，是GBDT的一种优化。但是相对GBDT， XGBoot主要在以下几个方面做了优化:

1. XGBoost是GBDT的一种工程实现方式，在GBDT的理论推导中，是利用一阶泰勒近似得到了GBDT本质上就是拟合损失函数的负梯度，但是XGBoot是利用到了一阶和二阶信息。**二阶信息保证了模型训练的更准确收敛的更快**。
2. GBDT中只是利用回归树来作为他的基分类器，但是XGBoost中还添加了线性分类器。并且在XGBoost的目标函数中添加了正则项来约束最后学习到的模型。
3. XGBoost在训练的过程中支持列抽样，类似于随机森林可以选择部分特征。这样不仅可以减少过拟合的风险还可以减少计算量。
4. XGBoot是**支持并行的**，这也是最主要优于GBDT的一点。<font color=nblue>XGBoost的并行并不是体现在tree的粒度上，而是体现在特征的粒度上。决策树学习最耗时的一个步骤就是对特征的排序，因为要确定最佳的分割点。但是XGBoost在训练之前预先对数据进行排序，然后保存为block结构，后面的迭代中重复使用这个结构，这就大大减少了计算量。在进行节点的分裂时，要计算每个特征的增益，最后选择大的增益去做分类，那么这里就可以开多线程来进行特征的增益计算。</font>
5. 对于缺失样本，XGBoot可以自动学习出他的裂变方向。缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个。如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树。
6. **可并行的近似直方图算法**。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

<font color=red>**为什么要使用二阶导数信息：二阶信息本身就能让梯度收敛更快更准确**</font>

### **7.2 XGBoost优化目标和公式推导**

**目标函数**

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200705105815125.png)

其中正则项控制着模型的复杂度，包括了叶子节点数目T和leaf score的L2模的平方：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200705110151184.png)

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200727162904593.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fhd3NlZHJmMTIzbGFsYQ==,size_16,color_FFFFFF,t_70,style="transform:rotate(90deg)")



![在这里插入图片描述](https://img-blog.csdnimg.cn/2020072716290532.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Fhd3NlZHJmMTIzbGFsYQ==,size_16,color_FFFFFF,t_70)



### 7.3 XGBoost防止过拟合的方法

- 引入正则化
- 在结点分裂时，采用类似于随机森林的列抽样
- 直接控制参数的复杂度，包括max_depth、min_child_weight gamma
- add randomness来使得对训练对噪声鲁棒。 包括subsample colsample_bytree。或者也可以减小步长 eta，但是需要增加num_round，来平衡步长因子的减小。
- early stop等

### 7.4 XGBoost的特征重要性的计算

XGBoost根据增益情况计算出来选择哪个特征作为分割点,而某个特征的重要性就是它在所有树中出现的次数之和。

### 7.5 XGBoost的特征并行化是怎么做的

决策树的学习最耗时的一个步骤就是对特征值进行排序，在进行节点分裂时需要计算每个特征的增益，最终选增益大的特征做分裂，各个特征的增益计算可开启多线程进行。而且可以采用并行化的近似直方图算法进行节点分裂。

### 7.6 XGB和LGB的区别

只想到三点，特征排序，特征切分和直方图和全排序   

1）更快的训练速度和更高的效率：LightGBM使用基于直方图的算法。   

2）直方图做差加速：一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。   

3）更低的内存占用：使用离散的箱子(bins)保存并替换连续值导致更少的内存占用。   

4）更高的准确率(相比于其他任何提升算法)：它通过leaf-wise分裂方法（在当前所有叶子节点中选择分裂收益最大的节点进行分裂，如此递归进行，很明显leaf-wise这种做法容易过拟合，因为容易陷入比较高的深度中，因此需要对最大深度做限制，从而避免过拟合。）产生比level-wise分裂方法（对每一层所有节点做无差别分裂，可能有些节点的增益非常小，对结果影响不大，但是xgboost也进行了分裂，带来了务必要的开销）更复杂的树，这就是实现更高准确率的主要因素。然而，它有时候或导致过拟合，但是我们可以通过设置|max-depth|参数来防止过拟合的发生。

5）大数据处理能力：相比于XGBoost，由于它在训练时间上的缩减，它同样能够具有处理大数据的能力。   

6）支持并行学习。   

7）局部采样：对梯度大的样本（误差大）保留，对梯度小的样本进行采样，从而使得样本数量降低，提高运算速度。

### 7.7 XGB的缺点

- XGBoosting采用预排序，在迭代之前，对结点的特征做预排序，遍历选择最优分割点，数据量大时，贪心法耗时，LightGBM方法采用histogram算法，占用的内存低，数据分割的复杂度更低。

- XGBoosting采用level-wise生成决策树，同时分裂同一层的叶子，从而进行多线程优化，不容易过拟合，但很多叶子节点的分裂增益较低，没必要进行跟进一步的分裂，这就带来了不必要的开销；LightGBM采用深度优化，leaf-wise生长策略，**每次从当前叶子中选择增益最大的结点进行分裂，循环迭代**，但会生长出更深的决策树，产生过拟合，因此引入了一个阈值进行限制，防止过拟合。

  ![img](https://upload-images.jianshu.io/upload_images/4559317-9832206c15393ff7.png)

  ![img](https://upload-images.jianshu.io/upload_images/4559317-c7405d6a116e69ea.png)





### 7.8 XGB常问面试题总结

> 1. [XGB面试题-上](https://mp.weixin.qq.com/s?__biz=Mzg2MjI5Mzk0MA==&mid=2247484181&idx=1&sn=8d0e51fb0cb974f042e66659e1daf447&chksm=ce0b59cef97cd0d8cf7f9ae1e91e41017ff6d4c4b43a4c19b476c0b6d37f15769f954c2965ef&scene=21#wechat_redirect)
> 2. [XGB面试题-下](https://mp.weixin.qq.com/s?__biz=Mzg2MjI5Mzk0MA==&mid=2247484193&idx=1&sn=81ff5a898e2f22357aab9e3742f1cc22&chksm=ce0b59faf97cd0ec095ec8b1e0d7521fb3ec6dea829dbd8a3960ed7ed5c3c0fe7d9e34649074&scene=21#wechat_redirect)

#### 1. 介绍一下XGB

首先需要说一说GBDT，它是一种基于boosting增强策略的加法模型，训练的时候采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。

XGBoost对GBDT进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行和默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。

#### 2. XGB与GBDT的不同

- 二阶泰勒展开式，引入了二阶信息
- 基分类器除了CART分类器，还引入了线性分类器。并且目标函数中加入了正则化
- 支持列抽样，可以有效的防止过拟合，并且减少了计算量
- 可以自动学习缺失值的分裂方向
- 支持并行化。XGB的并行化不是在tree的粒度而是在特征值的粒度上。先对特征值进行排序并保存为block结构。以后就可以重复使用这个block结构

#### 3. XGBoost为什么使用泰勒二阶展开

- **精准性**：相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数
- **可扩展性**：损失函数支持自定义，只需要新的损失函数二阶可导。

#### 4. XGB为什么可以并行训练

- XGBoost的并行，并不是说每棵树可以并行训练，XGB本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。
- XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。

#### 5. XGB为什么快

- **分块并行**：训练前每个特征按特征值进行排序并存储为Block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点
- **候选分位点**：每个特征采用常数个分位点作为候选分割点
- **CPU cache 命中优化**： 使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的Buffer中。
- **Block 处理优化**：Block预先放入内存；Block按列进行解压缩；将Block划分到不同硬盘来提高吞吐

#### 6. XGB是如何防止过拟合的

XGBoost在设计时，为了防止过拟合做了很多优化，具体如下：

- **目标函数添加正则项**：叶子节点个数+叶子节点权重的L2正则化
- **列抽样**：训练的时候只用一部分特征（不考虑剩余的block块即可）
- **子采样**：每轮计算可以不使用全部样本，使算法更加保守
- **shrinkage**: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间

#### 7. XGB是如何处理缺失值的

XGBoost模型的一个优点就是允许特征存在缺失值。对缺失值的处理方式如下：

- 在特征k上寻找最佳 split point 时，不会对该列特征 missing 的样本进行遍历，而只对该列特征值为 non-missing 的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找 split point 的时间开销。
- 在逻辑实现上，为了保证完备性，会将该特征值missing的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。
- 如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。

#### 8. XGBoost中叶子结点的权重如何计算出来



![img](https://mmbiz.qpic.cn/mmbiz_png/90dLE6ibsg0fDfLgXV02BLFJ9eaFEJB0ERQaHDopzOeSvCyaPGicmHqArjzlJYDejcTs9YJoAFdAqwyVrdpUPZQA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

![img](https://mmbiz.qpic.cn/mmbiz_png/90dLE6ibsg0fDfLgXV02BLFJ9eaFEJB0EURBYpwF4xF4x2lLh7BroeKUjRqk17VXpkZqPEjaskia4kiazjs9nyg0A/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

#### 9. XGBoost中的一棵树的停止生长条件

- 当新引入的一次分裂所带来的增益Gain<0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。
- 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。
- 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。

#### 10. XGB是如何处理数据不平衡问题的

对于不平衡的数据集，例如用户的购买行为，肯定是极其不平衡的，这对XGBoost的训练有很大的影响，XGBoost有两种自带的方法来解决：

第一种，如果你在意AUC，采用AUC来评估模型的性能，那你可以通过设置scale_pos_weight来平衡正样本和负样本的权重。例如，当正负样本比例为1:10时，scale_pos_weight可以取10；scale_pos_weight的本质是改变了数据的权重，增大了少数样本的权重。

第二种，如果你在意概率(预测得分的合理性)，你不能重新平衡数据集(会破坏数据的真实分布)，应该设置max_delta_step为一个有限数字来帮助收敛（基模型为LR时有效）。

**除此之外，还可以通过上采样、下采样、SMOTE算法或者自定义代价函数的方式解决正负样本不平衡的问题。**



#### 11. LR和GBDT的区别



#### 12. XGB是如何对树进行剪枝的

- 在目标函数中增加了正则项：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度。
- 在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂。
- 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂。
- XGBoost 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。

 

#### 13. XGBoost如何选择最佳分裂点？ 

​        XGBoost在训练前预先将特征按照特征值进行了排序，并存储为block结构，以后在结点分裂时可以重复使用该结构。
​        因此，可以采用特征并行的方法利用多个线程分别计算每个特征的最佳分割点，根据每次分裂后产生的增益，最终选择增益最大的那个特征的特征值作为最佳分裂点。
​        如果在计算每个特征的最佳分割点时，对每个样本都进行遍历，计算复杂度会很大，这种全局扫描的方法并不适用大数据的场景。XGBoost还提供了一种直方图近似算法，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，极大提升了结点分裂时的计算效率。

#### 14. XGBoost的Scalable性如何体现

- **基分类器的scalability**：弱分类器可以支持CART决策树，也可以支持LR和Linear。
- **目标函数的scalability**：支持自定义loss function，只需要其一阶、二阶可导。有这个特性是因为泰勒二阶展开，得到通用的目标函数形式。
- **学习方法的scalability**：Block结构支持并行化，支持 Out-of-core计算。

#### 15. XGB是如何评价一个特征的重要性的

- **weight** ：该特征在所有树中被用作分割样本的特征的总次数。
- **gain** ：该特征在其出现过的所有树中产生的平均增益。
- **cover** ：该特征在其出现过的所有树中的平均覆盖范围。





## 8. 各个集成学习算法之间的区别

### 8.1 GBDT和提升树的区别

这两种模型就是在生成新的训练数据时采用了不同的方法。**对于梯度提升树，其学习流程与提升树类似只是不再使用残差作为新的训练数据而是使用损失函数的负梯度作为新的新的训练数据的y值。**但是如果GBDT采用平方损失作为损失函数，其梯度就又是残差。

### 8.2 Adaboost和GBDT的区别

- Adaboost在每迭代一次以后，会对数据的权重做更新，被分错的数据会增大其权重，分对的数据会减少其权重；GBDT不会改变数据集的权重。
- Adaboost会对每个分类器添加一个权重，同样GBDT也可以给每个弱分类器添加一个权重。他们的添加权重的目的是不一样的。Adaboost的权重在训练过程种更新来标定每个训练器的重要性。但是GBDT是为了防止过拟合，同时也是减少其学习率，使得学习的结果变的更加可靠。
- Adaboost是通过更新数据集的权重来迭代的训练基分类器；但是对于GBDT，当其误差是均方误差的时候，他每次拟合的就是残差，也就是利用残差来更新数据集，当其损失函数是其他的损失的时候，每一个弱分类器拟合的就是损失函数的负梯度。

### 8.3 XGBoost和GBDT的区别



![在这里插入图片描述](https://img-blog.csdnimg.cn/20200601220639833.png)



1. XGBoost是GBDT的一种工程实现方式，在GBDT的理论推导中，是利用一阶泰勒近似得到了GBDT本质上就是拟合损失函数的负梯度，但是XGBoot是利用到了一阶和二阶信息。二阶信息保证了模型训练的更准确收敛的更快。
2. GBDT中只是利用回归树来作为他的基分类器，但是XGBoost中还添加了线性分类器。并且在XGBoost的目标函数中添加了正则项来约束最后学习到的模型。
3. XGBoost在训练的过程中支持列抽样，类似于随机森林可以选择部分特征。这样不仅可以减少过拟合的风险还可以减少计算量。
4. XGBoot是支持并行的，这也是最主要优于GBDT的一点。XGBoost的并行并不是体现在tree的粒度上，而是体现在特征的粒度上。决策树学习最耗时的一个步骤就是对特征的排序，因为要确定最佳的分割点。但是XGBoost在训练之前预先对数据进行排序，然后保存为block结构，后面的迭代中重复使用这个结构，这就大大减少了计算量。在进行节点的分裂时，要计算每个特征的增益，最后选择大的增益去做分类，那么这里就可以开多线程来进行特征的增益计算。
5. 对于缺失样本，XGBoot可以自动学习出他的裂变方向。缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个。如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树。
6. 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点。

### 8.4 Adaboost和随机森林的区别

1. Adaboost是boosting算法，随机森林是bagging算法
2. Adaboost是每次调整训练集中样本点的权重来训练下一个分类器，但是随机森林是又放回的抽取N个数据集来组成新的训练集来训练弱分类器
3. Adaboost中每个弱分类器是有权重的，但是随机森林的各个分类器的权重是一样的，最后采取投票的方式得到最后的额结果
4. Adaboost的各个弱分类器是串行训练的，但是随机森林可以并行训练
5. 随机森林是支持列抽样的。

### **8.5 GBDT和随机森林的区别**

1）随机森林采用的bagging思想，而GBDT采用的boosting思想。

2）组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成。

3）组成随机森林的树可以并行生成；而GBDT只能是串行生成。

4）对于最终的输出结果，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来。

5）随机森林对异常值不敏感；GBDT对异常值非常敏感。

6）随机森林是通过减少模型方差提高性能；GBDT是通过减少模型偏差提高性能。

## 9. 关于集成学习的面试题集锦

**1. bagging和boosting的区别**

**2. XGBoost和GBDT的区别**

**3. GBDT的原理和常用调参参数**

先用一个初始值去学习一棵树,然后在叶子处得到预测值以及预测后的残差,之后的树则基于之前树的残差不断的拟合得到，从而训练出一系列的树作为模型。

n_estimators基学习器的最大迭代次数，learning_rate学习率，max_lead_nodes最大叶子节点数,max_depth树的最大深度,min_samples_leaf叶子节点上最少样本数。

**GBDT中的小trick：**

GBDT中采用shrinkage来设置步长，这样可以有效避免过拟合。

**GBDT适用场景**

GBDT几乎可用于所有回归问题（线性/非线性），GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。



## 10. SVM

**10.1 什么是SVM？**SVM和LR有什么区别

**支持向量机为一个二分类模型,它的基本模型定义为特征空间上的间隔最大的线性分类器。而它的学习策略为最大化分类间隔, 最终可转化为凸二次规划问题求解。**SVM中引入核函数的本质就是针对线性不可分的数据集，寻求一种可以在低纬度进行特征计算然后映射到高纬度进行分类的方法。但是其本质还是针对线性不可分的数据集，映射到高纬度后寻求一个空间几何平面对其进行分割。

**SVM和LR的区别**

- LR是参数模型,SVM为非参数模型。（这里的参数和非参数指的是否服从某个参数分布，而非模型中的参数）
- LR采用的损失函数为logistical loss，而SVM采用的是hinge loss。
- 在学习分类器的时候, SVM利用其对偶性对原有目标函数进行转化，原有模型的复杂度和样本的维度有关，但是在转化后只是考虑与分类最相关的少数支持向量点。LR的模型相对简单, 在进行大规模线性分类时比较方便。

> **LR为什么不使用核函数：因为在SVM中核函数的本质是只计算支持向量，但是LR中会考虑所有的点，所有点都要两两计算，计算量太过庞大。**

**10.2 SVM中什么时候用线性核什么时候用高斯核?**

- 当数据的特征提取的较好,所包含的信息量足够大,很多问题是线性可分的那么可以采用线性核。
- 若特征数较少,样本数适中,对于时间不敏感,遇到的问题是线性不可分的时候可以使用高斯核来达到更好的效果。

**10.3 SVM中的软间隔和硬间隔**

![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552625881758_43F38C088CC6F7D987008FA92A07D44D)

![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552625892868_7E387DF316B75555F4C18CA19208AF66)

软间隔和硬间隔的差别就是有没有引入松弛变量

**10.4 SVM中为什么要引入对偶问题**

- 一是方便核函数的引入；
- 二是原问题的求解复杂度与特征的维数相关，而转成对偶问题后只与支持向量的个数有关。

> **由于SVM的变量个数为支持向量的个数，相较于特征位数较少，因此转为对偶问题。通过拉格朗日算子法使带约束的优化目标转为不带约束的优化函数，使得W和b的偏导数等于零，带入原来的式子，再通过转成对偶问题。**

**10.5 核函数的种类和适用情形**（这个回答是有错误的）

**线性核、多项式核、高斯核**

- 线性核：样本数量多且维度高；样本数量很多
- 高斯核：样本数量较少且维数不是很高，对时间不敏感

> 为什么高斯核可以映射无穷维度：因为把泰勒公式带入高斯核函数将得到一个无穷维度的映射

**10.6 SVM的损失函数**

合页损失函数

![img](https://uploadfiles.nowcoder.com/images/20190315/311436_1552628049041_E97F977CF363620D9944FCD1001BCA6B)

## 11. L1正则化和L2正则化

- L1是模型各个参数的绝对值之和,L2为各个参数平方和的开方值。
- L1更趋向于产生少量的特征,其它特征为0,最优的参数值很大概率出现在坐标轴上,从而导致产生稀疏的权重矩阵；而L2会选择更多的矩阵，但是这些矩阵趋向于0。

**为什么一般利用L2解决过拟合问题而非L1**

因为L1是一个绝对值求和的过程，在反向传播的过程中会涉及到求导，置零，解方程。但是如果给出一个绝对值方程，上述三者就会失效，求最小值就会有很大的麻烦

同样的对于 L1 和 L2 损失函数的选择，也会碰到同样的问题，所以最后大家一般用 L2 损失函数而不用 L1 损失函数的原因就是：**因为计算方便！**可以直接求导获得取最小值时各个参数的取值。此外还有一点，**用 L2 一定只有一条最好的预测线，L1 则因为其性质可能存在多个最优解**。当然 L1 损失函数主要就是**鲁棒性 (Robust) 更强，对异常值更不敏感**。



> L1和L2的差别，为什么一个让绝对值最小，一个让平方最小，会有那么大的差别呢？看导数一个是1一个是w便知, 在靠近0附近, L1以匀速下降到零, 而L2则完全停下来了. 这说明L1是将不重要的特征(或者说, 重要性不在一个数量级上)尽快剔除, L2则是把特征贡献尽量压缩最小但不至于为零.

L2平方项是个圆圈，防止过拟合找到最优化，L1是个正方形歪放在坐标轴，选出少量特征。

# 高频面试题

#### 1. 给定前序和中序遍历写出后序遍历

```C++
// 首先重构二叉树，然后递归得到后序遍历
class Solution {
public:
    TreeNode* buildTree(vector<int>& preorder, vector<int>& inorder) {
        int pres=0,ins=0;
        int pree=int(preorder.size()-1),ine=int(inorder.size()-1);
        return myBT(inorder,preorder,pres,pree,ins,ine);
    }

    TreeNode* myBT(vector<int>& inorder, vector<int>& preorder,int pres, int pree, int ins, int ine){
        TreeNode* root;int mid;
        if(pres > pree || ins > ine )
            return NULL;
        root = new TreeNode(preorder[pres]);
        for(int i=0;i<inorder.size();i++)
            if(inorder[i]==preorder[pres]){
                mid = i;
                break;
            }
        root->left = myBT(inorder,preorder,pres+1,pres+mid-ins,ins,mid-1);
        root->right = myBT(inorder,preorder,pree-(ine-mid)+1,pree,mid+1,ine);
        return root;
    }
};

// 后序遍历，递归版本
void laterOrder(TreeNode* root){
    if(root ==NULL) return;
    laterOrder(root->left);
    laterOrder(root->right);
    cout << root->val << endl;
}
```

#### 2. 求最大路径和

```C++
// 求最大路径和
/*给定一颗二叉树，求可以穿过根节点的最大路径和*/
class Solution {
public:
    int res=INT_MIN; // 全局变量
    int maxPathSum(TreeNode* root) {
        getmax(root);
        return res;
    }
    
    int getmax(TreeNode* root){
        if(!root) 
            return 0;
        // 计算左边分支最大值，左边分支如果为负数还不如不选择
        int left=max(getmax(root->left),0);
        // 计算右边分支最大值，右边分支如果为负数还不如不选择
        int right=max(getmax(root->right),0);
        // left->root->right 作为路径与历史最大值做比较
        res=max(res,root->val+left+right);
        // 返回经过root的单边最大分支给上游
        return max(left,right)+root->val;
    }
};
```

#### 3. 全排列问题

```C++
class Solution{
public:
	void swap(vector<int> &a, int i, int j){
        int temp = 0;
        temp = a[i];
        a[i] = a[j];
        a[j] = temp
    }
    
    void save(vector<int> &a, int q){
        for(int i=0; i<q+1; i++){
            cout << a[i] << " ";
        }
        cout << endl;
    }
    
    void main_(vector<int>&a, int p, int q){
        if(p==q){
            save(a, q+1);
        }
        
        else{
            for(int i=p; i<=q; i++){
                swap(a, i, p);
                main_(a, p+1, q);
                swap(a, i, p);
            }
        }   
    }
};
```

#### 4. 顺时针打印数组

```C++
class Solution {
public:
    vector<int> spiralOrder(vector<vector<int>>& matrix) {
        vector<int> ans;
        int R,C;
        if(!(R=matrix.size()) || !(C=matrix[0].size())){
            return ans;
        }

        int top=0,left=0,right=C-1,bottom=R-1;
        while(ans.size() < R*C){
            //遍历上边
            for(int i=left;i<=right;++i) ans.push_back(matrix[top][i]);
            //遍历右边
            for(int i=top+1;i<bottom;++i) ans.push_back(matrix[i][right]);
            //遍历下边
            for(int i=right;i>=left && bottom > top;--i) ans.push_back(matrix[bottom][i]);
            //遍历左边
            for(int i=bottom-1;i>top && left < right;--i) ans.push_back(matrix[i][left]);
            top++;bottom--;
            left++;right--;
        }
        return ans;
    }
};
```

#### 5. 最长回文字符串

```C++
// 给定一个字符串 s ，找到其中最长的回文子序列，并返回该序列的长度。可以假设 s 的最大长度为 1000 。
//输入："bbbab"
//输出：4

int longestPalindromeSubseq(string s) {
    int n = s.size();
    // dp 数组全部初始化为 0
    vector<vector<int>> dp(n, vector<int>(n, 0));
    // base case
    for (int i = 0; i < n; i++)
        dp[i][i] = 1;
        
    // 反着遍历保证正确的状态转移
    for (int i = n - 1; i >= 0; i--) {
        for (int j = i + 1; j < n; j++) {
            // 状态转移方程
            if (s[i] == s[j])
                dp[i][j] = dp[i + 1][j - 1] + 2;
            else
                dp[i][j] = max(dp[i + 1][j], dp[i][j - 1]);
        }
    }
    // 整个 s 的最长回文子串长度
    return dp[0][n - 1];
}
```

